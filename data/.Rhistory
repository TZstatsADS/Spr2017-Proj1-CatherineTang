# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=as.vector(diag(1/(word.count+0.01)))%*%as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(docs.df[i,-ncol(docs.df)],
sentences=as.character(sentences),
word.count,
#emotions
sent.id=1:length(sentences)
)
)
}
}
emotions=get_nrc_sentiment(sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
speech.df=tbl_df(sentence.list)%>%
select(sentences, anger:trust)
sentence.list=NULL
for(i in 1:nrow(docs.df)){
sentences=sent_detect(docs.df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=as.vector(diag(1/(word.count+0.01)))%*%as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(docs.df[i,-ncol(docs.df)],
sentences=as.character(sentences),
word.count,
#emotions
sent.id=1:length(sentences)
)
)
}
}
emotions=get_nrc_sentiment(sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
speech.df=tbl_df(sentence.list)%>%
select(sentences, anger:trust)
View(sentence.list)
emotions=get_nrc_sentiment(sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
as.matrix(emotions)
emotions=get_nrc_sentiment(sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
word.count=word_count(sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";"))))
emotions=get_nrc_sentiment(sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
word.count=word_count(sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
as.matrix(emotions)
diag(1/(word.count+0.01))
diag(1/(word.count+0.01))
word.count
1/(word.count+0.01)
diag(0.0006548746)
?diag
diag(10)
1/(word.count+0.01)
word.count
sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
sent_detect(docs.df$text[1],
endmarks = c("?", ".", "!", "|",";"))
sentence.list=NULL
docs1<-Corpus(DirSource(folder.path,encoding = "UTF-8"))
docs1.df<-as.data.frame(docs1)
for(i in 1:nrow(docs1.df)){
sentences=sent_detect(docs1.df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01)) %*% as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(docs1.df[i,-ncol(docs1.df)],
sentences=as.character(sentences),
word.count,
#emotions
sent.id=1:length(sentences)
)
)
}
}
sent_detect(docs1.df$text[1],
endmarks = c("?", ".", "!", "|",";"))
emotions=get_nrc_sentiment(sent_detect(docs1.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
word.count=word_count(sent_detect(docs1.df$text[1],
endmarks = c("?", ".", "!", "|",";")))
as.matrix(emotions)
word.count
1/(word.count+0.01)
diag(0.0006548746)
diag(1/(word.count+0.01))
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
sentence.list=NULL
docs1<-Corpus(DirSource(folder.path,encoding = "UTF-8"))
docs1.df<-as.data.frame(docs1)
for(i in 1:nrow(docs1.df)){
sentences=sent_detect(docs1.df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01)) %*% as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(docs1.df[i,-ncol(docs1.df)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
#sentence.list=
#  sentence.list%>%
#  filter(!is.na(word.count))
speech.df=tbl_df(sentence.list)%>%
select(sentences, anger:trust)
speech.df=as.data.frame(speech.df)
as.character(speech.df$sentences[apply(speech.df[,-1], 2, which.max)])
heatmap.2(cor(sentence.list%>%select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
heatmap.2(cor(sentence.list),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
heatmap.2(cor(select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
heatmap.2(cor(select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
heatmap.2(cor(sentence.list%>%select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
heatmap.2(cor(sentence.list%>%filter(type=="inaug")select(anger:trust)),
heatmap.2(cor(sentence.list%>%filter(type=="inaug")%>%select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
heatmap.2(cor(sentence.list%>%dplyr::select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
sentence.list=NULL
docs1<-Corpus(DirSource(folder.path,encoding = "UTF-8"))
docs1.df<-as.data.frame(docs1)
for(i in 1:nrow(docs1.df)){
sentences=sent_detect(docs1.df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01)) %*% as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(docs1.df[i,-ncol(docs1.df)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
View(sentence.list)
heatmap.2(cor(sentence.list%>%select(sentences,anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
sentence.list=NULL
docs1<-Corpus(DirSource(folder.path,encoding = "UTF-8"))
docs1.df<-as.data.frame(docs1)
for(i in 1:nrow(docs1.df)){
sentences=sent_detect(docs1.df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01)) %*% as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(docs1.df[i,-ncol(docs1.df)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
View(speech.list)
View(topic.p.mat)
View(sentence.list)
heatmap.2(cor(sentence.list%>%filter(type=="inaug")%>%select(anger:trust)),
scale = "none",
col = bluered(100), , margin=c(6, 6), key=F,
trace = "none", density.info = "none")
sentence.list%>%
filter(File=="BarackObama",
type=="inaug",
word.count<=3)%>%
select(sentences)
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
sentence.list=NULL
for(i in 1:nrow(speech.list)){
sentences=sent_detect(speech.list$fulltext[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01))%*%as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(speech.list[i,-ncol(speech.list)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
sentence.list=NULL
docs1<-Corpus(DirSource(folder.path,encoding = "UTF-8"))
docs1.df<-as.data.frame(docs1)
for(i in 1:nrow(docs1.df)){
sentences=sent_detect(docs1.df$text[i],
endmarks = c("?", ".", "!", "|",";"))
if(length(sentences)>0){
emotions=get_nrc_sentiment(sentences)
word.count=word_count(sentences)
# colnames(emotions)=paste0("emo.", colnames(emotions))
# in case the word counts are zeros?
emotions=diag(1/(word.count+0.01)) %*% as.matrix(emotions)
sentence.list=rbind(sentence.list,
cbind(docs1.df[i,-ncol(docs1.df)],
sentences=as.character(sentences),
word.count,
emotions,
sent.id=1:length(sentences)
)
)
}
}
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
sentence.list=
sentence.list%>%
filter(!is.na(word.count))
library("wordcloud")
library("tidytext")
library("ggplot2")
library("rvest")
library("tibble")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("reshape2")
folder.path="../data/InauguralSpeeches/"
speeches=list.files(path = folder.path, pattern = "*.txt")
prez.out=substr(speeches, 6, nchar(speeches)-4)
length.speeches=rep(NA, length(speeches))
docs<-Corpus(DirSource(folder.path,encoding = "UTF-8"))
docs<-tm_map(docs, stripWhitespace)
docs<-tm_map(docs, content_transformer(tolower))
docs<-tm_map(docs, removeNumbers)
docs<-tm_map(docs, removeWords, stopwords("english"))
docs<-tm_map(docs, removeWords, c("can","will","shall","nation","day","now","may","among","would","america","must","with","without","might","let","american","upon"))
docs<-tm_map(docs, removeWords, character(0))
docs<-tm_map(docs, removePunctuation)
docs <- tm_map(docs,stemDocument)
D<-as.data.frame(docs)
D$docs<-paste(prez.out)
dtm <- DocumentTermMatrix(docs)
dtmss <- removeSparseTerms(dtm, 0.15)
rownames(dtm) <- paste(prez.out)
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm  <- dtm[rowTotals> 0, ]
dtm.mat<-as.matrix(dtm)
dtm.df<-as.data.frame(dtm.mat)
dtm.df<-data.frame(File=dtm$dimnames$Docs,dtm.df)
library("xlsx")
filename<-"../data/InaugurationInfo.xlsx"
presid.info<-read.xlsx(file = filename,1,encoding="UTF-8",header=T)
presid.term<-merge(presid.info,dtm.df,by="File",all=T)
presid.term<-presid.term[order(presid.term$Time),]
presid.term$File <- factor(presid.term$File, levels=presid.term$File, ordered=TRUE)
dtm.df<-presid.term[,-c(2,3,4,5,6)]
dtm.mat<-as.matrix(dtm.df)
h.freq<-findFreqTerms(dtm,lowfreq = 160)
length(h.freq)
print(h.freq)
freq<-sort(colSums(as.matrix(dtm)),decreasing = T)
wordcloud(names(freq),freq,max.words = length(h.freq),colors=brewer.pal(7,"Purples"),rot.per = F)
# Frequency for the high ~ words
obj<-freq[freq >= 160]
h.f<-data.frame(term=names(obj),freq=obj)
ggplot(h.f, aes(x=term,y=freq)) +
geom_bar(stat="identity",fill="Violet")+
labs(title="Mostly Used Words",x="Term",y="Frenquency")+
theme(axis.text.x  = element_text(angle=45, size=10,vjust=0.5),
plot.title = element_text(colour = "black", face = "bold",size = 20,vjust = 1))+
geom_text(aes(label=freq), vjust=-0.2)
m1<-dtm.mat[,h.freq]
cn<-colnames(m1)
rn<-dtm.mat[,"File"]
dtm.freq.mat<-matrix(as.numeric(m1),byrow=T,nrow=58)
colnames(dtm.freq.mat)<-cn
rownames(dtm.freq.mat)<-rn
corr.freq<-cor(dtm.freq.mat)
coln<-colnames(corr.freq)
corr.freq<-melt(corr.freq)
ggplot(data = corr.freq) +
geom_tile(aes(x = Var1, y = Var2, fill = value)) +
theme_classic() +
theme(axis.ticks = element_blank(),
axis.line = element_blank(),
axis.text.x  = element_text(angle=45, vjust=0.5)) +
labs(title="Covariance between the mostly used words ",x="Words",y="Words")+
scale_x_discrete(labels = coln) +
scale_y_discrete(labels = coln) +
scale_fill_gradient2('value',
low = "blue", mid = "white", high = "purple")
library(factoextra)
km.res=kmeans(t(dtmss), iter.max=200,4)
fviz_cluster(km.res,
stand=T, repel= TRUE,
data = t(dtmss),
show.clust.cent=TRUE)
# Compute hierarchical clustering
d <- dist(t(dtmss), method = "euclidean")
res <- hcut(d, k = 4, stand = TRUE)
fviz_dend(res, rect = TRUE, cex = 0.8,
k_colors = c("purple","violet","blue","skyblue"))
# high frequency words apperence in each docs
coln<-colnames(dtm.freq.mat)
rown<-rownames(dtm.freq.mat)
ggplot(data = melt(dtm.freq.mat)) +
geom_tile(aes(x = Var1, y = Var2, fill = value),color = "white") +
labs(title="Term frequence in speeches", x="Time",y="Terms")+
theme_classic() +
theme(axis.ticks = element_blank(),
axis.line = element_blank(),
axis.text.x  = element_text(angle=60, vjust=0.5) ) +
scale_fill_gradient(low = "white",high = "purple")+
scale_x_discrete(labels =rown) +
scale_y_discrete(labels =coln)
# Words counting
ggplot(presid.term, aes(x=File,y=Words)) +
geom_bar(stat="identity",fill="violet")+
theme(axis.text.x  = element_text(angle=45, vjust=0.5))+
geom_text(aes(label=Words), vjust=-0.2)
library(gridExtra)
a<-ggplot(presid.term,aes(x=File,y=Words,group=1))+
geom_point(aes(size=Words) ,col="violet") +
geom_smooth(formula=y~x,method="loess",col="blue")+
theme(axis.text.x  = element_text(angle=60, vjust=0.5))
b<-ggplot(presid.term,aes(x=File,y=Words,group=1))+
geom_point(aes(size=Words) ,col="violet") +
geom_smooth(formula=y~x,method="lm",col="blue")+
theme(axis.text.x  = element_text(angle=60, vjust=0.5))
grid.arrange(a,b,nrow=2)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 1000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm, k, method="Gibbs", control=list(nstart=nstart,
seed = seed, best=best,
burnin = burnin, iter = iter,
thin=thin))
#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
table(c(1:k, ldaOut.topics))
write.csv(ldaOut.topics,file=paste("../output/LDAGibbs",k,"DocsToTopics.csv"))
#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
write.csv(ldaOut.terms,file=paste("../output/LDAGibbs",k,"TopicsToTerms.csv"))
#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("../output/LDAGibbs",k,"TopicProbabilities.csv"))
topic.p.mat<-as.matrix(topicProbabilities)
terms.beta=ldaOut@beta
terms.beta=scale(terms.beta)
topics.terms=NULL
for(i in 1:k){
topics.terms=rbind(topics.terms, ldaOut@terms[order(terms.beta[i,], decreasing = TRUE)[1:7]])
}
topics.terms
ldaOut.terms
topics.hash=c("Topic1","Topic2","Topic3","Topic4","Topic5","Topic6","Topic7","Topic8","Topic9","Topic10")
docs$ldatopic=as.vector(ldaOut.topics)
docs$ldahash=topics.hash[ldaOut.topics]
colnames(topicProbabilities)=topics.hash
docs.df=cbind(docs, topicProbabilities)
docs.df$docs<-paste(prez.out)
docs.df<-docs.df[presid.term$File,]
par(mar=c(1,1,1,1))
topic.summary=tbl_df(docs.df)%>%
select(docs, Topic1:Topic10)%>%
group_by(docs)%>%
summarise_each(funs(mean))
topic.summary=as.data.frame(topic.summary)
rownames(topic.summary)=prez.out
topic.plot=c(1:length(topics.hash))
print(topics.hash[topic.plot])
heatmap.2(t(as.matrix(topic.summary[,topic.plot+1])),
scale = "column", key=F,
col=colorRampPalette(c("white","purple")),
cexRow = 0.9, cexCol = 0.9, margins = c(8, 8),
trace = "none", density.info = "none")
presid.summary=tbl_df(docs.df)%>%
#filter(type=="inaug", File%in%sel.comparison)%>%
select(docs, Topic1:Topic10)%>%
group_by(docs)%>%
summarise_each(funs(mean))
presid.summary=as.data.frame(presid.summary)
rownames(presid.summary)=as.character(prez.out)
set.seed(19)
km.res=kmeans(scale(presid.summary[,-1]), iter.max=200,
5)
fviz_cluster(km.res,
stand=T, repel= TRUE,
data = presid.summary[,-1],
show.clust.cent=FALSE)
set.seed(1999)
d <- dist(dtmss, method = "euclidean")
res <- hcut(d, k = 5, stand = TRUE)
fviz_dend(res, rect = TRUE, cex = 0.5,
k_colors = c("purple","violet","blue","plum","skyblue"))
library("openNLP")
require("NLP")
docs.df<-docs.df[presid.term$File,]
# Compose the tagPOS function
tagPOS <-  function(text.var, pos_tag_annotator, ...) {
s <- as.String(text.var)
## Set up the POS annotator if missing (for parallel)
PTA <- Maxent_POS_Tag_Annotator()
## Need sentence and word token annotations.
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- Annotation(1L, "sentence", 1L, nchar(s))
a2 <- annotate(s, word_token_annotator, a2)
a3 <- annotate(s, PTA, a2)
## Determine the distribution of POS tags for word tokens.
a3w <- a3[a3$type == "word"]
pos_tag <- unlist(lapply(a3w$features, "[[", "POS"))
## Extract token/POS pairs (all of them): easy.
pos_term <- list(term = s[a3w], tag = pos_tag)
return (pos_term)
}
acqTag<-NULL
for(i in 1:58){
acqTag[i] <- tagPOS(docs.df$text[i])
sapply(acqTag,function(x) {res = sub(x); res[!grepl("\\s",res)]} )
}
